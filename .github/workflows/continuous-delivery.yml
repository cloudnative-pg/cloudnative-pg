name: continuous-delivery

on:
  push:
  pull_request:
  workflow_dispatch:
    inputs:
      depth:
        description: 'Depth (push, pull_request, main, schedule)'
        required: true
        default: 'main'
      limit:
        description: 'Limit to the specified engines list (local, eks, aks, gke)'
        required: false
  schedule:
    - cron:  '0 1 * * *'

env:
  GOLANG_VERSION: "1.14.x"
  KIND_VERSION: "v0.9.0"

defaults:
  run:
    shell: 'bash -Eeuo pipefail -x {0}'

jobs:

  pre_job:
    runs-on: ubuntu-20.04
    outputs:
      should_skip: ${{ steps.skip_check.outputs.should_skip }}
    steps:
      - id: skip_check
        uses: fkirc/skip-duplicate-actions@v3.4.0
        with:
          concurrent_skipping: 'same_content'
          skip_after_successful_duplicate: 'true'
          do_not_skip: '["pull_request", "workflow_dispatch", "schedule"]'

  generate-jobs:
    name: Generate jobs for E2E tests
    needs: pre_job
    if: ${{ needs.pre_job.outputs.should_skip != 'true' }}
    runs-on: ubuntu-20.04
    outputs:
      image: ${{ steps.set-image.outputs.image }}
      localMatrix: ${{ steps.generate-jobs.outputs.localMatrix }}
      localEnabled: ${{ steps.generate-jobs.outputs.localEnabled }}
      eksMatrix: ${{ steps.generate-jobs.outputs.eksMatrix }}
      eksEnabled: ${{ steps.generate-jobs.outputs.eksEnabled }}
      aksMatrix: ${{ steps.generate-jobs.outputs.aksMatrix }}
      aksEnabled: ${{ steps.generate-jobs.outputs.aksEnabled }}
      gkeMatrix: ${{ steps.generate-jobs.outputs.gkeMatrix }}
      gkeEnabled: ${{ steps.generate-jobs.outputs.gkeEnabled }}
    steps:
      -
        name: Wait for build to succeed
        uses: fountainhead/action-wait-for-check@v1.0.0
        id: wait-for-build
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          checkName: Build containers
          ref: ${{ github.event.pull_request.head.sha || github.sha }}
      -
        name: Get image name from artifact
        uses: dawidd6/action-download-artifact@v2
        with:
          name: build-image
          path: path/to/artifact
          github_token: ${{secrets.GITHUB_TOKEN}}
          workflow: build.yml
          workflow_conclusion: success
      -
        name: Set image name to use
        id: set-image
        run: |
          image=$(cat path/to/artifact/image.txt)
          echo "::set-output name=image::${image}"
      -
        name: Checkout code
        if: steps.wait-for-build.outputs.conclusion == 'success'
        uses: actions/checkout@v2
      -
        name: Docker meta
        if: steps.wait-for-build.outputs.conclusion == 'success'
        id: docker-meta
        uses: crazy-max/ghaction-docker-meta@v1
        with:
          images: quay.io/enterprisedb/cloud-native-postgresql-testing
          tag-semver: |
            {{version}}
      -
        name: Generate Jobs
        if: steps.wait-for-build.outputs.conclusion == 'success'
        id: generate-jobs
        shell: bash
        run: |
          python .github/e2e-matrix-generator.py \
            -m '${{ github.event.inputs.depth || github.event_name }}' \
            -l '${{ github.event.inputs.limit }}'

  e2e-local:
    name: Run E2E on local executors
    if: needs.generate-jobs.outputs.localEnabled == 'true'
    needs:
      - generate-jobs
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.generate-jobs.outputs.localMatrix) }}
    runs-on: ubuntu-20.04
    env:
      K8S_VERSION: "${{ matrix.k8s_version }}"
      POSTGRES_IMG: "${{ matrix.postgres_img }}"
      E2E_PRE_ROLLING_UPDATE_IMG: "${{ matrix.postgres_pre_img }}"

      DOCKER_SERVER: quay.io/enterprisedb
      DOCKER_USERNAME: ${{ secrets.QUAY_USERNAME }}
      DOCKER_PASSWORD: ${{ secrets.QUAY_TOKEN }}

      DEBUG: "true"
      BUILD_IMAGE: "false"
      CONTROLLER_IMG: ${{ needs.generate-jobs.outputs.image }}
      E2E_DEFAULT_STORAGE_CLASS: standard
      LOG_DIR: ${{ github.workspace }}/kind-logs/
      DOCKER_REGISTRY_MIRROR: https://mirror.gcr.io
    steps:
      -
        name: Checkout code
        uses: actions/checkout@v2
      -
        name: Install Go
        uses: actions/setup-go@v2
        with:
          go-version: ${{ env.GOLANG_VERSION }}
      -
        name: Clean up all the left overs in docker
        run:
          docker system prune -a -f
      -
        name: Run Kind End-to-End tests
        run:
          make e2e-test-kind
      -
        name: Archive Kind logs
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: kind-logs-${{ matrix.id }}
          path: kind-logs/
          retention-days: 7
      -
        name: Archive e2e failure contexts
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: test-failure-contexts-${{ matrix.id }}
          path: |
            tests/*/out/
          retention-days: 7
          if-no-files-found: ignore

  e2e-aks:
    name: Run E2E on Microsoft AKS
    if: needs.generate-jobs.outputs.aksEnabled == 'true'
    needs:
      - generate-jobs
    strategy:
      fail-fast: false
      max-parallel: 15
      matrix:
         ${{ fromJSON(needs.generate-jobs.outputs.aksMatrix) }}
    runs-on: ubuntu-20.04
    env:
      K8S_VERSION: "${{ matrix.k8s_version }}"
      POSTGRES_IMG: "${{ matrix.postgres_img }}"
      E2E_PRE_ROLLING_UPDATE_IMG: "${{ matrix.postgres_pre_img }}"

      DOCKER_SERVER: quay.io/enterprisedb
      DOCKER_USERNAME: ${{ secrets.QUAY_USERNAME }}
      DOCKER_PASSWORD: ${{ secrets.QUAY_TOKEN }}

      DEBUG: "true"
      BUILD_IMAGE: "false"
      CONTROLLER_IMG: ${{ needs.generate-jobs.outputs.image }}
      E2E_DEFAULT_STORAGE_CLASS: rook-ceph-block
    steps:
      -
        name: Checkout code
        uses: actions/checkout@v2
      -
        name: Install Go
        uses: actions/setup-go@v2
        with:
          go-version: ${{ env.GOLANG_VERSION }}
      -
        name:  Install ginkgo
        run: |
          go install -mod=readonly github.com/onsi/ginkgo/ginkgo
      -
        name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      -
        name: Install kubectl
        uses: azure/setup-kubectl@v1
        with:
          version: v${{ env.K8S_VERSION }}
      -
        name: Create AKS cluster
        run: |
            az extension add --name aks-preview
            az account set --subscription "Cloud Native Development"
            az aks create --resource-group ${{ secrets.AZURE_RESOURCEGROUP }} \
              --name ${{ secrets.AZURE_RESOURCENAME }}-${{ github.run_number }}-$( echo ${{ matrix.id }} | tr -d '_.-' ) \
              --node-count 3 -k v${K8S_VERSION} --generate-ssh-keys --enable-addons monitoring \
              --workspace-resource-id ${{ secrets.AZURE_WORKSPACE_RESOURCE_ID }} \
              --aks-custom-headers EnableAzureDiskFileCSIDriver=true
            az aks get-credentials --resource-group ${{ secrets.AZURE_RESOURCEGROUP }} \
              --name ${{ secrets.AZURE_RESOURCENAME }}-${{ github.run_number }}-$( echo ${{ matrix.id }} | tr -d '_.-' )
      -
        # Azure is slow in provisioning disks, and we can't wait two minutes
        # every time we create a pod, otherwise all the tests will time out.
        # We set up a few large disks now, we run Rook on top of them and we
        # use rook to get the small PV we use in the tests.
        # It can still take a while to deploy rook.
        name: Set up Rook
        run: |
          if [ "${K8S_VERSION}" \> "1.20.1" ]; then
            STORAGECLASSNAME=default
          else
            STORAGECLASSNAME=managed-csi
          fi
          GO111MODULE=on go get github.com/mikefarah/yq/v4
          ROOK_BASE_URL=https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph #wokeignore:rule=master
          kubectl apply -f ${ROOK_BASE_URL}/crds.yaml
          kubectl apply -f ${ROOK_BASE_URL}/common.yaml
          kubectl apply -f ${ROOK_BASE_URL}/operator.yaml
          curl ${ROOK_BASE_URL}/cluster-on-pvc.yaml | \
            sed '/^ *#/d;/^ *$/d' | \
            yq e ".spec.storage.storageClassDeviceSets[].volumeClaimTemplates[].spec.resources.requests.storage = \"50Gi\" |
              .spec.storage.storageClassDeviceSets[].volumeClaimTemplates[].spec.storageClassName = \"${STORAGECLASSNAME}\" |
              .spec.mon.volumeClaimTemplate.spec.storageClassName = \"${STORAGECLASSNAME}\" " - | \
            kubectl apply -f -
          ITER=0
          MAX_ITER=120
          while true; do
            output=$( kubectl get deploy -n rook-ceph -l app=rook-ceph-osd --no-headers -o name )
            if [[ $(wc -w <<< $output)  == 3 ]]; then
              break
            fi

            (( ++ITER ))
            if [[ ${ITER} -ge ${MAX_ITER} ]]; then
              echo "Time out waiting for Rook OSDs deployments"
              exit 1
            fi
            echo "$(wc -w <<< $output) out of 3 Rook OSDs deployments exist, waiting 10s (${ITER}/${MAX_ITER})."
            sleep 10
          done
          echo "Waiting for Rook OSDs to be available"
          kubectl wait deploy -n rook-ceph --for condition=available --timeout 480s -l app=rook-ceph-osd
          kubectl apply -f ${ROOK_BASE_URL}/csi/rbd/storageclass.yaml
      -
        name: Run E2E tests
        run: hack/e2e/run-e2e.sh
      -
        name: Archive e2e failure contexts
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: test-failure-contexts-${{ matrix.id }}
          path: |
            tests/*/out/
          retention-days: 7
          if-no-files-found: ignore
      -
        name: Clean up
        if: always()
        uses: azure/CLI@v1
        with:
          inlineScript: |
            az account set --subscription "Cloud Native Development"
            az aks delete --resource-group ${{ secrets.AZURE_RESOURCEGROUP }} -y \
              --name ${{ secrets.AZURE_RESOURCENAME }}-${{ github.run_number }}-$( echo ${{ matrix.id }} | tr -d '_.-' )

  e2e-eks:
    name: Run E2E on Amazon EKS
    if: needs.generate-jobs.outputs.eksEnabled == 'true'
    needs:
      - generate-jobs
    strategy:
      fail-fast: false
      matrix:
         ${{ fromJSON(needs.generate-jobs.outputs.eksMatrix) }}
    runs-on: ubuntu-20.04
    env:
      K8S_VERSION: "${{ matrix.k8s_version }}"
      POSTGRES_IMG: "${{ matrix.postgres_img }}"
      E2E_PRE_ROLLING_UPDATE_IMG: "${{ matrix.postgres_pre_img }}"

      DOCKER_SERVER: quay.io/enterprisedb
      DOCKER_USERNAME: ${{ secrets.QUAY_USERNAME }}
      DOCKER_PASSWORD: ${{ secrets.QUAY_TOKEN }}

      DEBUG: "true"
      BUILD_IMAGE: "false"
      CONTROLLER_IMG: ${{ needs.generate-jobs.outputs.image }}
      E2E_DEFAULT_STORAGE_CLASS: gp2

      AWS_REGION: eu-central-1
    steps:
      -
        name: Set cluster name
        run: |
          echo "CLUSTER_NAME=cnp-test-${{ github.run_number }}-$( echo ${{ matrix.id }} | tr -d '_.-' )" >> $GITHUB_ENV
      -
        name: Checkout code
        uses: actions/checkout@v2
      -
        name: Install Go
        uses: actions/setup-go@v2
        with:
          go-version: ${{ env.GOLANG_VERSION }}
      -
        name:  Install ginkgo
        run: |
          go install -mod=readonly github.com/onsi/ginkgo/ginkgo
      -
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      -
        name: Install eksctl
        run: |
          mkdir -p "$HOME/.local/bin"
          curl -sL "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" \
            | tar xz -C $HOME/.local/bin
          echo "$HOME/.local/bin" >> $GITHUB_PATH
      -
        name: Configure EKS setup
        run: |
          envsubst < hack/e2e/eks-cluster.yaml.template > hack/e2e/eks-cluster.yaml
      -
        name: Setup EKS
        run: |
          eksctl create cluster -f hack/e2e/eks-cluster.yaml
      -
        name: Setup log archiving via fluentd and insights
        run: |
          curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml \
            | sed "s/{{cluster_name}}/${{ env.CLUSTER_NAME }}/;s/{{region_name}}/${{ env.AWS_REGION }}/" \
            | kubectl apply -f -
      -
        name: Run E2E tests
        run: hack/e2e/run-e2e.sh
      -
        name: Archive e2e failure contexts
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: test-failure-contexts-${{ matrix.id }}
          path: |
            tests/*/out/
          retention-days: 7
          if-no-files-found: ignore
      -
        name: Clean up
        if: always()
        run: |
          while true; do
              output=$( eksctl delete cluster -n ${{ env.CLUSTER_NAME }} -r "${{ env.AWS_REGION }}" --wait 2>&1 )
              status=$?
              if [[ $status == 0 ]]; then
                  echo "EKS cluster deleted"
                  break
              fi
              if ( grep "ResourceNotFoundException: No cluster found for name: ${{ env.CLUSTER_NAME }}" <<< "$output" ); then
                  echo "EKS cluster doesn't exist, nothing to remove"
                  break
              fi
              echo "Failed deleting cluster ${{ env.CLUSTER_NAME }}, retrying"
              sleep 5
          done

  e2e-gke:
    name: Run E2E on Google GKE
    if: needs.generate-jobs.outputs.gkeEnabled == 'true'
    needs:
      - generate-jobs
    strategy:
      fail-fast: false
      max-parallel: 10
      matrix:
        ${{ fromJSON(needs.generate-jobs.outputs.gkeMatrix) }}
    runs-on: ubuntu-20.04
    env:
      K8S_VERSION: "${{ matrix.k8s_version }}"
      POSTGRES_IMG: "${{ matrix.postgres_img }}"
      E2E_PRE_ROLLING_UPDATE_IMG: "${{ matrix.postgres_pre_img }}"

      DOCKER_SERVER: quay.io/enterprisedb
      DOCKER_USERNAME: ${{ secrets.QUAY_USERNAME }}
      DOCKER_PASSWORD: ${{ secrets.QUAY_TOKEN }}

      DEBUG: "true"
      BUILD_IMAGE: "false"
      CONTROLLER_IMG: ${{ needs.generate-jobs.outputs.image }}
      E2E_DEFAULT_STORAGE_CLASS: standard

      ZONE: europe-west3-a
    steps:
      -
        name: Checkout code
        uses: actions/checkout@v2
      -
        name: Install Go
        uses: actions/setup-go@v2
        with:
          go-version: ${{ env.GOLANG_VERSION }}
      -
        name:  Install ginkgo
        run: |
          go install -mod=readonly github.com/onsi/ginkgo/ginkgo
      -
        name: Set cluster name
        run: |
          # GKE cluster names rules:
          # only lowercase alphanumerics and '-' allowed, must start with a letter and end with an alphanumeric,
          # and must be no longer than 40 characters
          # We need to shorten the name and lower the case
          SHORT_ID=$( echo ${{ matrix.id }} | tr -d '_.-' | tr '[:upper:]' '[:lower:]')
          echo "CLUSTER_NAME=cnp-test-${{ github.run_number }}-${SHORT_ID}" >> $GITHUB_ENV
      -
        name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@master #wokeignore:rule=master
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT }}
          export_default_credentials: true
      -
        name: Install kubectl
        run: gcloud components install --quiet kubectl
      -
        name: Create GKE cluster
        run: |
          # We may go over the amount of API requests allowed
          # by Google when creating all the clusters at the same time.
          # We give a few attempts at creating the cluster before giving up
          for i in `seq 1 5`; do
            if gcloud container clusters create ${{ env.CLUSTER_NAME }} \
              --num-nodes=3 \
              --cluster-version=${{ env.K8S_VERSION }} \
              --zone=${{ env.ZONE }} \
              --disk-size=20 \
              --machine-type=e2-standard-2
            then
              exit 0
            fi
            echo "Couldn't create the cluster. Retrying in 100s."
            sleep 100
          done
          echo "Couldn't create the cluster. Failing."
          exit 1
      -
        name: Get GKE kubeconfig credentials
        uses: google-github-actions/get-gke-credentials@main
        with:
          cluster_name: ${{ env.CLUSTER_NAME }}
          location: ${{ env.ZONE }}
          credentials: ${{ secrets.gcp_credentials }}
          use_auth_provider: true
      -
        name: Run E2E tests
        run: hack/e2e/run-e2e.sh
      -
        name: Archive e2e failure contexts
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: test-failure-contexts-${{ matrix.id }}
          path: |
            tests/*/out/
          retention-days: 7
          if-no-files-found: ignore
      -
        name: Clean up
        if: always()
        run: |
          gcloud container clusters delete ${{ env.CLUSTER_NAME }} --zone=${{ env.ZONE }} --quiet
          # Disks are not automatically deleted when the cluster is removed and
          # their name only contains the run number, so we can't find the actual cluster from
          # the matrix that created them. We delete all the disks of a run that are not
          # owned by anyone, without worrying to much if some other job deleted them first.
          IDS=$(gcloud compute disks list --filter="name~cnp-test-${{github.run_number}} AND zone:${{env.ZONE}} AND -users:*" --format="value(id)")
          for ID in ${IDS}
          do
            gcloud compute disks delete --zone "${{env.ZONE}}" --quiet "${ID}" || true
          done
