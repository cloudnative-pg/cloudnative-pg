# Scheduling

In Kubernetes, scheduling is the process responsible for placing a new pod on
the best node possible, based on several criteria.

!!! Seealso "Kubernetes documentation"
    See the
    [Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/)
    for more information on scheduling, including all the available policies.
    We assume you're familiar with concepts like affinity,
    anti-affinity, node selectors, and so on.

You can control how to schedule the CloudNativePG cluster's instances
through the [`affinity`](cloudnative-pg.v1.md#postgresql-cnpg-io-v1-AffinityConfiguration)
section in the definition of the cluster. This section supports:

- Pod affinity and anti-affinity
- Node selectors
- Tolerations

!!! Info
    CloudNativePG doesn't support pod templates for finer control
    on the scheduling of workloads. While they were part of the initial concept,
    the development team decided to postpone their introduction in a newer
    version of the API, most likely v2 of CNPG.

## Pod affinity and anti-affinity

Kubernetes allows you to control the nodes for a pod to schedule (*affinity*) or
not to schedule (*anti-affinity*). This schedule is based on the actual workloads already
running in those nodes.
This control is technically known as *inter-pod affinity/anti-affinity*.

CloudNativePG by default configures the cluster's instances
preferably on different nodes, resulting in the following `affinity` definition:

```yaml
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: postgresql
                operator: In
                values:
                  - cluster-example
          topologyKey: kubernetes.io/hostname
        weight: 100
```

The definition is a result of the following `Cluster` spec:

```yaml
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3
  imageName: ghcr.io/cloudnative-pg/postgresql:16.1

  affinity:
    enablePodAntiAffinity: true #default value
    topologyKey: kubernetes.io/hostname #defaul value
    podAntiAffinityType: preferred #default value

  storage:
    size: 1Gi
```

Therefore, Kubernetes *prefers* to schedule a 3-node PostgreSQL cluster over 3
different nodes, resources permitting.

You can change the behavior by adjusting the settings.

You can set `podAntiAffinityType` to `required`, which results in
`requiredDuringSchedulingIgnoredDuringExecution` being used instead of
`preferredDuringSchedulingIgnoredDuringExecution`. Be aware that such a
strong requirement might result in pending instances if resources aren't
available. This behavior is an expected condition when using
[Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) <!-- wokeignore:rule=master -->
for automated horizontal scaling of a Kubernetes cluster.

!!! Seealso "Inter-pod affinity and anti-affinity"
    More information is in the
    [Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity).

Another possible value for `topologyKey` in a cloud environment is
`topology.kubernetes.io/zone`. This value helps to ensure pods are spread across
availability zones and not just nodes. See
[Well-Known Labels, Annotations and Taints](https://kubernetes.io/docs/reference/labels-annotations-taints/)
for more options.

You can disable the operator's generated anti-affinity policies by setting
`enablePodAntiAffinity` to `false`.

Additionally, if you need finer control, you can specify a
list of custom pod affinity or anti-affinity rules using the
`additionalPodAffinity` and `additionalPodAntiAffinity` configuration
attributes. These rules are added to the ones generated by the operator,
if enabled, or passed transparently otherwise.

!!! Note
    You have to pass to `additionalPodAntiAffinity` or `additionalPodAffinity`
    the whole content of `podAntiAffinity` or `podAffinity` that's expected by the
    `Pod` spec. The following YAML is an example of having only one
    instance of PostgreSQL running on every worker node, regardless of which
    PostgreSQL cluster they belong to.

```yaml
    additionalPodAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: postgresql
            operator: Exists
            values: []
        topologyKey: "kubernetes.io/hostname"
```

## Node selection through `nodeSelector`

Kubernetes allows `nodeSelector` to provide a list of labels, defined as
key-value pairs, to select the nodes on which a pod can run. Specifically,
for the pod to be scheduled and run, the node must have each indicated key-value pair as labels.

Similarly, CloudNativePG allows you to define a `nodeSelector` in the
`affinity` section, so that you can request a PostgreSQL cluster to run only
on nodes that have those labels.

## Tolerations

Kubernetes allows you to specify (through `taints`) for a node to repel
all pods not explicitly tolerating (through `tolerations`) their `taints`.

So, by setting a proper set of `tolerations` for a workload matching a specific
node's `taints`, Kubernetes scheduler takes into consideration the
tainted node, while deciding on which node to schedule the workload.
Tolerations can be configured for all the pods of a cluster in the
`.spec.affinity.tolerations` section. This section accepts the usual Kubernetes syntax
for tolerations.

!!! Seealso "Taints and Tolerations"
    For more information on taints and tolerations, see the
    [Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).
